import streamlit as st
import joblib
import numpy as np
import pandas as pd
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import torch
import json
from sentence_transformers.util import cos_sim
import os

# Set page title and icon
st.set_page_config(page_title="Misinformation Detector", page_icon="üîç")

# Load the trained model and artifacts
@st.cache_resource
def load_artifacts():
    try:
        artifacts = joblib.load("production_model_v2.pkl")
        return {
            'model': artifacts['model'],
            'scaler': artifacts.get('scaler', None),
            'features': artifacts.get('features', None),
            'metadata': artifacts.get('metadata', None)
        }
    except Exception as e:
        st.error(f"Failed to load model artifacts: {str(e)}")
        st.stop()

# Load NLP models for automatic feature generation
@st.cache_resource
def load_nlp_models():
    try:
        # Sentiment analysis for stance detection
        sentiment_pipe = pipeline(
            "text-classification",
            model="distilbert-base-uncased-finetuned-sst-2-english",
            device=0 if torch.cuda.is_available() else -1
        )

        # Semantic similarity model
        similarity_model = SentenceTransformer('all-MiniLM-L6-v2')

        return {
            'sentiment': sentiment_pipe,
            'similarity': similarity_model
        }
    except Exception as e:
        st.error(f"Failed to load NLP models: {str(e)}")
        st.stop()

# Load Phase 5 claims and embeddings
@st.cache_resource
def load_phase5_data():
    with open("phase5_claims.json", "r") as f:
        claims = json.load(f)
    embeddings = np.load("phase5_embeddings.npy")
    return claims, embeddings

# Initialize all models
artifacts = load_artifacts()
model = artifacts['model']
scaler = artifacts['scaler']
nlp_models = load_nlp_models()
phase5_claims, phase5_embeddings = load_phase5_data()

# --- UI Layout ---
st.title("üîç Misinformation Detector")
st.markdown("""
    *Enter a claim to automatically check if it's reliable or misinformation!*
""")

# Input field with a unique key
if "current_claim_input" not in st.session_state:
    st.session_state.current_claim_input = ""

claim_text = st.text_area("**Claim to Analyze:**", 
                         value=st.session_state.current_claim_input,
                         placeholder="e.g., 'COVID-19 vaccines were tested in trials'",
                         height=100,
                         key="current_claim_input")

# Predict button
if st.button("**Analyze Claim**", type="primary"):
    if not claim_text.strip():
        st.warning("‚ö†Ô∏è Please enter a claim!")
    else:
        with st.spinner("Analyzing claim..."):
            try:
                # Encode user claim
                claim_embedding = nlp_models["similarity"].encode(claim_text)

                # Check for semantic match with reliable facts
                reliable_facts = [
                    "smoking causes lung cancer",
                    "the earth revolves around the sun",
                    "wearing seatbelts reduces injury in car crashes",
                    "climate change is caused by greenhouse gas emissions",
                    "regular exercise improves heart health",
                    "the human body needs water to survive",
                    "hiv is transmitted through unprotected sex and blood contact",
                    "antibiotics do not treat viral infections",
                    "the covid-19 vaccine reduces severe illness and death",
                    "washing hands prevents the spread of germs",
                    "reading improves cognitive skills",
                    "drinking clean water is essential for good health",
                    "exposure to secondhand smoke is harmful",
                    "excessive sugar intake increases the risk of diabetes",
                    "polio vaccine eradicated polio in many countries",
                    "wearing a helmet reduces the risk of head injury",
                    "global warming is a result of carbon emissions",
                    "seat belts save lives",
                    "hand sanitizers kill most germs",
                    "sunlight is a source of vitamin D"
                ]
                reliable_embeddings = [nlp_models["similarity"].encode(fact) for fact in reliable_facts]
                similarities = [np.dot(claim_embedding, emb) for emb in reliable_embeddings]
                if max(similarities) > 0.75:
                    prediction = 0
                    proba = [0.95, 0.05]
                    stance_score = 0.5
                    semantic_sim = 0.8
                    source_quality = 0.9
                    credibility_index = (stance_score * 0.4 + semantic_sim * 0.3 + source_quality * 0.3)
                    discrepancy = abs(stance_score - semantic_sim)
                    source_boost = np.log1p(source_quality * 10)

                else:
                    # Phase 5 similarity matching
                    similarities = [float(np.dot(claim_embedding, emb)) for emb in phase5_embeddings]
                    best_idx = int(np.argmax(similarities))
                    best_sim = similarities[best_idx]
                    matched = phase5_claims[best_idx]
                    SIMILARITY_THRESHOLD = 0.65

                    # Keyword check
                    keywords = ["covid", "vaccine", "climate", "smoking", "election", "virus", "cancer", "mask"]
                    keyword_match = any(k in claim_text.lower() and k in matched["text"].lower() for k in keywords)

                    if best_sim >= SIMILARITY_THRESHOLD and keyword_match:
                        stance_score = matched["stance_score"]
                        semantic_sim = matched["semantic_similarity"]
                        source_quality = matched["source_quality"]
                        st.info(f"üîé Matched with similar verified claim: \"{matched['text']}\"")
                    else:
                        # Fallback: generate features live
                        st.warning("‚ö†Ô∏è No similar verified claim found. Using fallback model with estimated features.")

                        # 1. Sentiment analysis (less biased weighting)
                        sentiment_result = nlp_models['sentiment'](claim_text)[0]
                        stance_score = 0.6 if sentiment_result['label'] == 'NEGATIVE' else 0.4
                        stance_score *= sentiment_result['score']

                        # 2. Conservative similarity
                        semantic_sim = 0.3

                        # 3. Conservative source quality
                        source_quality = 0.8  # trusted fallback

                        st.info(f"üß† Estimated stance: {sentiment_result['label']} ({sentiment_result['score']:.2f})")

                    # Feature engineering
                    credibility_index = (stance_score * 0.4 + semantic_sim * 0.3 + source_quality * 0.3)
                    discrepancy = abs(stance_score - semantic_sim)
                    source_boost = np.log1p(source_quality * 10)

                    features = np.array([
                        stance_score,
                        semantic_sim,
                        source_quality,
                        credibility_index,
                        discrepancy,
                        source_boost
                    ]).reshape(1, -1)

                    if scaler is not None:
                        features = scaler.transform(features)

                    proba = model.predict_proba(features)[0]
                    prediction = model.predict(features)[0]

                # Show result
                st.subheader("üìä Result")
                if prediction == 0:
                    st.success(f"‚úÖ **Reliable** (Confidence: {proba[0] * 100:.1f}%)")
                else:
                    st.error(f"‚ùå **Misinformation** (Confidence: {proba[1] * 100:.1f}%)")

                # Probability breakdown
                st.write("**Details:**")
                st.write(f"- Reliable probability: `{proba[0] * 100:.1f}%`")
                st.write(f"- Misinformation probability: `{proba[1] * 100:.1f}%`")

                # Feature analysis
                with st.expander("**How this was determined**"):
                    st.write("The system analyzed your claim using these factors:")

                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("Stance Score", f"{stance_score:.2f}",
                                help="Higher = More opinionated (negative sentiment)")
                        st.metric("Semantic Similarity", f"{semantic_sim:.2f}",
                                help="Higher = More internally consistent")
                    with col2:
                        st.metric("Source Quality", f"{source_quality:.2f}",
                                help="Estimated reliability of typical sources")
                        st.metric("Credibility Index", f"{credibility_index:.2f}",
                                help="Combined measure of reliability")

            except Exception as e:
                st.error(f"Analysis failed: {str(e)}")
                st.error("Please try again with a different claim")

# Footer
st.markdown("---")
st.caption("Built with Streamlit | ML Course Project")